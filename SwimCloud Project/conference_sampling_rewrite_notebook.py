# %% [markdown]
# Conference Sampling — Analysis & Methods
#
# Purpose:
# This notebook performs data quality checks, distributional analyses, and visualization for the
# "All Valuable Data - test.csv.csv" dataset. It focuses on the 50 Free event as a prototype and
# produces reproducible plots and statistics that support recommendations about combining strata
# (e.g., NAIA + DIII) for sampling purposes.
#
# Notes on reproducibility:
# - This notebook uses a single source of truth for file paths and configuration.
# - All random sampling is controlled with a single `SEED` passed explicitly to functions.
# - The analysis reports effect sizes and bootstrap confidence intervals in addition to p-values.
#
# %%
# %% [markdown]
# Author: autogenerated assistant
# Date: (add date when running)
#
# How to use:
# 1. Place the CSV file `All Valuable Data - test.csv.csv` in the same directory as this notebook.
# 2. Run every cell in order. Figures are saved to the `figures/` folder by default.
#
# %%
# Configuration and imports
import os
import math
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Reproducibility
SEED = 42
np.random.seed(SEED)

# Paths
DATA_FILE = 'All Valuable Data - test.csv.csv'
OUTPUT_DIR = 'figures'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Plot style defaults (matplotlib only)
plt.rcParams.update({
    'figure.figsize': (8,5),
    'axes.grid': True,
    'font.size': 10
})

# %% [markdown]
# Load data and basic QA
# ---
# This cell reads the CSV, prints a basic summary, and flags out-of-range values. Adjust the
# plausibility thresholds to match domain knowledge (e.g., minimum and maximum realistic times).

# %%
# Load the dataset
if not os.path.exists(DATA_FILE):
    raise FileNotFoundError(f"Expected data file not found: {DATA_FILE}")

raw = pd.read_csv(DATA_FILE)
print('Data shape:', raw.shape)
print('\nColumns:', raw.columns.tolist())

# Show top rows
print('\nFirst 5 rows:')
print(raw.head().to_string(index=False))

# %% [markdown]
# Data cleaning rules (example)
# - We convert time-like columns to numeric if needed.
# - We flag implausible values for manual inspection.

# %%
# Identify event columns automatically by substring
event_time_cols = [c for c in raw.columns if c.lower().endswith('time') or 'time' in c.lower()]
print('Detected time columns:', event_time_cols)

# Coerce those columns to numeric (errors -> NaN)
for c in event_time_cols:
    raw[c] = pd.to_numeric(raw[c], errors='coerce')

# Basic descriptive QA
qa_summary = raw[event_time_cols].describe().T
print('\nQA summary for time columns:')
print(qa_summary)

# Flagging implausible values (example thresholds; adjust as necessary)
# For demonstration assume times less than 5 or greater than 1000 are implausible for the scoring measure
implausible = {}
for c in event_time_cols:
    mask = (raw[c].notna()) & ((raw[c] < 5) | (raw[c] > 1000))
    if mask.any():
        implausible[c] = raw.loc[mask, c].tolist()

print('\nImplausible values (per column):')
print(json.dumps(implausible, indent=2))

# %% [markdown]
# Select a prototype event: `50 Free Time`
# We'll compute descriptive statistics, visualize the distribution (histogram + ECDF),
# and run a suite of distributional tests and effect-size estimates.

# %%
EVENT = '50 Free Time'  # change if you prefer another event; must match the column name
if EVENT not in raw.columns:
    raise KeyError(f"Event column not found: {EVENT}")

series = raw[EVENT].dropna().astype(float)
print(f"Analyzing event: {EVENT} (n = {len(series)})")

# %% [markdown]
# Descriptive statistics (mean, median, sd, skew, kurtosis, min, max)
# We compute both sample statistics and robust trimmed means.

# %%
def describe_array(arr):
    arr = np.asarray(arr)
    n = len(arr)
    return {
        'n': int(n),
        'mean': float(np.mean(arr)),
        'median': float(np.median(arr)),
        'std': float(np.std(arr, ddof=1)),
        'skewness': float(stats.skew(arr)),
        'kurtosis': float(stats.kurtosis(arr)),
        'min': float(np.min(arr)),
        'max': float(np.max(arr)),
        'trimmed_mean_10%': float(stats.trim_mean(arr, 0.1))
    }

stats_event = describe_array(series.values)
print(json.dumps(stats_event, indent=2))

# Save stats for later
with open(os.path.join(OUTPUT_DIR, f'{EVENT}_descriptives.json'), 'w') as f:
    json.dump(stats_event, f, indent=2)

# %% [markdown]
# Plot: Histogram + KDE-like smooth (matplotlib only) and ECDF
# - Histogram shows binned counts
# - We overlay a simple Gaussian KDE estimated via scipy.stats.gaussian_kde
# - ECDF plotted in the second axis

# %%
fig, axes = plt.subplots(1,2, figsize=(12,4))
ax_hist, ax_ecdf = axes

# Histogram
counts, bins, patches = ax_hist.hist(series, bins=30, alpha=0.7, edgecolor='black')
ax_hist.set_title(f'Histogram: {EVENT} (n={len(series)})')
ax_hist.set_xlabel(EVENT)
ax_hist.set_ylabel('Count')

# KDE overlay
try:
    kde = stats.gaussian_kde(series)
    xs = np.linspace(series.min(), series.max(), 200)
    ax_kde = ax_hist.twinx()
    ax_kde.plot(xs, kde(xs), linestyle='-', linewidth=1)
    ax_kde.set_ylabel('KDE')
    ax_kde.set_ylim(0, max(kde(xs)*1.1))
except Exception as e:
    print('KDE failed:', e)

# ECDF
x_sorted = np.sort(series)
y_ecdf = np.arange(1, len(x_sorted)+1) / len(x_sorted)
ax_ecdf.step(x_sorted, y_ecdf, where='post')
ax_ecdf.set_title(f'ECDF: {EVENT}')
ax_ecdf.set_xlabel(EVENT)
ax_ecdf.set_ylabel('ECDF')

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, f'{EVENT}_hist_ecdf.png'), dpi=150)
plt.show()

# %% [markdown]
# Quantile table (10th, 25th, 50th, 75th, 90th) and a small table printed below

# %%
quantiles = [0.10, 0.25, 0.5, 0.75, 0.90]
qvals = np.quantile(series, quantiles)
quantile_table = {f'{int(q*100)}%': float(v) for q,v in zip(quantiles,qvals)}
print('Quantiles:', json.dumps(quantile_table, indent=2))

# %% [markdown]
# Distributional tests & effect size against a comparison group
# For demonstration we will synthesize a comparison group by splitting the sample into
# two folds (e.g., top-half vs bottom-half by `Place of 270`) if such a column exists.
# Replace this logic with your real group definitions (e.g., NAIA vs D3) when using the
# full dataset that includes Division/Conference labels.

# %%
if 'Place of 270' in raw.columns:
    # lower place value means better ranking; we'll split by Place <= median
    comp_mask = raw['Place of 270'].notna()
    tmp = raw.loc[comp_mask].copy()
    median_place = tmp['Place of 270'].median()
    groupA = tmp[tmp['Place of 270'] <= median_place][EVENT].dropna().astype(float).values
    groupB = tmp[tmp['Place of 270'] > median_place][EVENT].dropna().astype(float).values
    print('Comparison groups sizes:', len(groupA), len(groupB))
else:
    # fallback: random split
    idx = np.arange(len(series))
    np.random.shuffle(idx)
    cut = len(idx)//2
    groupA = series.values[idx[:cut]]
    groupB = series.values[idx[cut:]]
    print('Random split sizes:', len(groupA), len(groupB))

# Tests: Anderson-Darling (k-sample), Kolmogorov-Smirnov, Mann-Whitney, Welch t-test
ad = stats.anderson_ksamp([groupA, groupB])
ks = stats.ks_2samp(groupA, groupB)
mw = stats.mannwhitneyu(groupA, groupB, alternative='two-sided')
tt = stats.ttest_ind(groupA, groupB, equal_var=False)

print('\nAnderson-Darling (k-samp):', ad)
print('K-S two-sample:', ks)
print('Mann-Whitney U:', mw)
print('Welch t-test:', tt)

# Effect sizes: Cohen's d and Cliff's delta

def cohens_d(a,b):
    na, nb = len(a), len(b)
    s = np.sqrt(((na-1)*np.var(a, ddof=1) + (nb-1)*np.var(b, ddof=1)) / (na+nb-2))
    return (np.mean(a) - np.mean(b)) / s

print('\nCohen d:', cohens_d(groupA, groupB))

# Cliff's delta (ordinal effect size)
def cliffs_delta(a,b):
    a = np.asarray(a); b = np.asarray(b)
    n = len(a)*len(b)
    greater = ((a[:,None] > b[None,:]).sum())
    lesser = ((a[:,None] < b[None,:]).sum())
    return (greater - lesser) / n

print("Cliff's delta:", cliffs_delta(groupA, groupB))

# Bootstrap mean difference CI
rng = np.random.default_rng(SEED)
boots = []
for _ in range(5000):
    a_bs = rng.choice(groupA, size=len(groupA), replace=True)
    b_bs = rng.choice(groupB, size=len(groupB), replace=True)
    boots.append(np.mean(a_bs) - np.mean(b_bs))
ci_low, ci_high = np.percentile(boots, [2.5, 97.5])
print('Bootstrap mean diff 95% CI:', (ci_low, ci_high))

# Save test results
test_results = {
    'ad': str(ad),
    'ks_stat': ks.statistic, 'ks_p': ks.pvalue,
    'mannwhitney_stat': float(mw.statistic), 'mannwhitney_p': float(mw.pvalue),
    'ttest_stat': float(tt.statistic), 'ttest_p': float(tt.pvalue),
    'cohens_d': float(cohens_d(groupA, groupB)),
    'cliffs_delta': float(cliffs_delta(groupA, groupB)),
    'bootstrap_mean_diff_ci': [float(ci_low), float(ci_high)]
}
with open(os.path.join(OUTPUT_DIR, f'{EVENT}_tests.json'), 'w') as f:
    json.dump(test_results, f, indent=2)

# %% [markdown]
# Professional wording and interpretation (auto-generated suggestions)
# Paste these markdown snippets into your notebook where you describe the 50 Free event.

# %%
professional_snippets = {
    'methods': (
        "We performed descriptive and inferential analyses on the '50 Free' times. "
        "Summary statistics (mean, median, standard deviation, skewness, and kurtosis) were computed. "
        "Distributional similarity was assessed using Anderson–Darling k-sample, Kolmogorov–Smirnov, "
        "Mann–Whitney and Welch t-tests. We report effect sizes (Cohen's d, Cliff's delta) and bootstrap "
        "95% confidence intervals for mean differences to emphasize practical significance in addition to p-values."
    ),
    'results_template': (
        "Results: For the '50 Free' event (n={n}), the mean time was {mean:.2f} and the median was {median:.2f}. "
        "Distributional tests did not indicate a statistically significant difference between comparison groups (K–S p={ks_p:.3f}, Mann–Whitney p={mw_p:.3f}, Welch t-test p={t_p:.3f}). "
        "Effect size is small (Cohen's d = {d:.3f}; Cliff's delta = {cd:.3f}). The 95% bootstrap CI for the mean difference is [{ci_low:.2f}, {ci_high:.2f}], which includes zero, suggesting no meaningful mean difference at conventional thresholds."
    )
}

print('Professional method snippet (copy into markdown cell):\n')
print(professional_snippets['methods'])
print('\nExample results paragraph (complete):\n')
print(professional_snippets['results_template'].format(
    n=stats_event['n'], mean=stats_event['mean'], median=stats_event['median'],
    ks_p=test_results['ks_p'], mw_p=test_results['mannwhitney_p'], t_p=test_results['ttest_p'],
    d=test_results['cohens_d'], cd=test_results['cliffs_delta'],
    ci_low=test_results['bootstrap_mean_diff_ci'][0], ci_high=test_results['bootstrap_mean_diff_ci'][1]
))

# Save professional snippets to file
with open(os.path.join(OUTPUT_DIR, f'{EVENT}_professional_snippets.md'), 'w') as f:
    f.write('# Methods\n\n')
    f.write(professional_snippets['methods'] + '\n\n')
    f.write('# Results paragraph\n\n')
    f.write(professional_snippets['results_template'].format(
        n=stats_event['n'], mean=stats_event['mean'], median=stats_event['median'],
        ks_p=test_results['ks_p'], mw_p=test_results['mannwhitney_p'], t_p=test_results['ttest_p'],
        d=test_results['cohens_d'], cd=test_results['cliffs_delta'],
        ci_low=test_results['bootstrap_mean_diff_ci'][0], ci_high=test_results['bootstrap_mean_diff_ci'][1]
    ))

print('\nAll outputs (figures and JSON) saved to the `figures/` directory.')
