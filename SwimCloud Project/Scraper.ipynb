{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129e3f36",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# SWIMMING FINA BIAS ANALYSIS - JUPYTER NOTEBOOK\n",
    "# Testing the hypothesis that breaststroke events have inflated FINA points\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # Swimming FINA Bias Analysis\n",
    "# \n",
    "# This notebook tests the hypothesis that certain swimming events (particularly breaststroke)\n",
    "# have systematically inflated FINA points compared to their actual competitive difficulty.\n",
    "# \n",
    "# ## Research Question\n",
    "# Do breaststroke events receive inflated FINA points relative to their percentile performance\n",
    "# compared to other strokes like freestyle?\n",
    "# \n",
    "# ## Methodology\n",
    "# - Stratified random sampling across NCAA Divisions 1-3 and NAIA\n",
    "# - Focus on men's SCY events: 100/200 Breast, 100/200 Free\n",
    "# - Statistical comparison using percentile-normalized FINA points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0036939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SwimScraper imported successfully\n",
      "Analysis started: 2025-08-29 19:53:09\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Setup and Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# SwimCloud scraping\n",
    "try:\n",
    "    from SwimScraper import SwimScraper as ss\n",
    "    print(\"‚úì SwimScraper imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå SwimScraper not found. Install with: pip install SwimScraper\")\n",
    "    raise\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24aad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample size: 50\n",
      "Priority events: ['100 Breast', '200 Breast', '100 Free', '200 Free']\n",
      "Division allocation will be computed dynamically based on rosters\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 2: Configuration and Constants (REPLACEMENT)\n",
    "\n",
    "# FINA Base Times for men's SCY (approximate - update with official values)\n",
    "FINA_BASE_TIMES = {\n",
    "    '50 Free': 17.63,      # Caeleb Dressel ‚Äî NCAA DI record :contentReference[oaicite:0]{index=0}\n",
    "    '100 Free': 39.83,     # Jordan Crooks ‚Äî NCAA DI record :contentReference[oaicite:1]{index=1}\n",
    "    '200 Free': 88.33,     # Luke Hobson ‚Äî recorded as 1:28.33 = 88.33 seconds :contentReference[oaicite:2]{index=2}\n",
    "    '100 Breast': 49.51,   # Julian Smith ‚Äî NCAA record :contentReference[oaicite:3]{index=3}\n",
    "    '200 Breast': 106.35,  # Leon Marchand ‚Äî NCAA record (1:46.35 = 106.35 s) :contentReference[oaicite:4]{index=4}\n",
    "    '100 Back': 43.20,     # Hubi Kos ‚Äî NCAA record :contentReference[oaicite:5]{index=5}\n",
    "    '100 Fly': 42.80,      # Caeleb Dressel ‚Äî NCAA record :contentReference[oaicite:6]{index=6}\n",
    "    '200 IM': 96.34        # Leon Marchand ‚Äî NCAA record (1:36.34 = 96.34 s) :contentReference[oaicite:7]{index=7}\n",
    "}\n",
    "\n",
    "\n",
    "# Analysis configuration\n",
    "TARGET_SAMPLE_SIZE = 50   # Flexible ‚Äî change this to rerun at different scales\n",
    "PRIORITY_EVENTS = ['100 Breast', '200 Breast', '100 Free', '200 Free']\n",
    "CURRENT_YEAR = 2024\n",
    "\n",
    "# Minimum per division to avoid zero-sampling\n",
    "MIN_PER_DIVISION = 10\n",
    "DEFAULT_ROSTER_SIZE = 25   # fallback if roster not returned\n",
    "D1_TIERS = 3               # stratify D1 into rank tiers\n",
    "\n",
    "print(f\"Target sample size: {TARGET_SAMPLE_SIZE}\")\n",
    "print(f\"Priority events: {PRIORITY_EVENTS}\")\n",
    "print(\"Division allocation will be computed dynamically based on rosters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf92fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3: Helper Functions\n",
    "def time_to_seconds(time_str):\n",
    "    \"\"\"Convert swimming time string (MM:SS.ss or SS.ss) to seconds\"\"\"\n",
    "    try:\n",
    "        if ':' in str(time_str):\n",
    "            parts = str(time_str).split(':')\n",
    "            minutes = float(parts[0])\n",
    "            seconds = float(parts[1])\n",
    "            return minutes * 60 + seconds\n",
    "        else:\n",
    "            return float(time_str)\n",
    "    except:\n",
    "        return 9999.99\n",
    "\n",
    "def calculate_fina_points(time_seconds, event):\n",
    "    \"\"\"Calculate FINA points using official formula: 1000 * (B/T)^3\"\"\"\n",
    "    if event not in FINA_BASE_TIMES or time_seconds <= 0 or time_seconds >= 999:\n",
    "        return None\n",
    "    \n",
    "    base_time = FINA_BASE_TIMES[event]\n",
    "    fina_points = 1000 * (base_time / time_seconds) ** 3\n",
    "    return round(fina_points, 2)\n",
    "\n",
    "def safe_api_call(func, *args, **kwargs):\n",
    "    \"\"\"Wrapper for API calls with error handling and debug logging\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        time.sleep(0.2)  # prevent SwimCloud blocking\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {func.__name__} failed. args={args}, kwargs={kwargs} -> {e}\")\n",
    "        return None\n",
    "\n",
    "# --- PATCH: collect swimmer data safely ---\n",
    "def collect_swimmer_data(swimmer, team_name, team_id, division):\n",
    "    \"\"\"Collect times for a single swimmer in priority events\"\"\"\n",
    "    if not swimmer or not isinstance(swimmer, dict):\n",
    "        print(f\"  [WARN] Invalid swimmer object on {team_name} ({team_id})\")\n",
    "        return None\n",
    "\n",
    "    swimmer_data = {\n",
    "        'swimmer_ID': swimmer.get('swimmer_ID'),\n",
    "        'swimmer_name': swimmer.get('swimmer_name', 'Unknown'),\n",
    "        'team_name': team_name,\n",
    "        'team_ID': team_id,\n",
    "        'division': division,\n",
    "        'times': {}\n",
    "    }\n",
    "\n",
    "    for event in PRIORITY_EVENTS:\n",
    "        times = safe_api_call(ss.getSwimmerTimes, swimmer_data['swimmer_ID'], event)\n",
    "        if not times:\n",
    "            print(f\"    [INFO] No times for {swimmer_data['swimmer_name']} ({event})\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # pick season best\n",
    "            season_best = min(times, key=lambda x: time_to_seconds(x.get('time', '99:99.99')))\n",
    "            time_seconds = time_to_seconds(season_best.get('time'))\n",
    "            if time_seconds < 999:\n",
    "                swimmer_data['times'][event] = {\n",
    "                    'time_str': season_best.get('time'),\n",
    "                    'time_seconds': time_seconds,\n",
    "                    'meet_name': season_best.get('meet_name', ''),\n",
    "                    'year': season_best.get('year', '')\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Processing times failed for {swimmer_data['swimmer_name']} ({event}): {e}\")\n",
    "            continue\n",
    "\n",
    "    return swimmer_data if swimmer_data['times'] else None\n",
    "\n",
    "print(\"‚úì Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c34e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Data Collection Functions (REPLACEMENT)\n",
    "\n",
    "def get_division_rosters(year=CURRENT_YEAR, gender=\"M\"):\n",
    "    \"\"\"Fetch all teams in each division and estimate roster sizes\"\"\"\n",
    "    divisions = { 'Division 1': [], 'Division 2': [], 'Division 3': [], 'NAIA': [] }\n",
    "\n",
    "    for div in divisions.keys():\n",
    "        teams = safe_api_call(ss.getCollegeTeams, division_names=[div])\n",
    "        if not teams:\n",
    "            continue\n",
    "\n",
    "        for t in teams:\n",
    "            team_id = t.get('team_ID')\n",
    "            team_name = t.get('team_name')\n",
    "            roster = safe_api_call(ss.getRoster, team=team_name, team_ID=team_id, gender=gender, year=year)\n",
    "            roster_size = len(roster) if roster else DEFAULT_ROSTER_SIZE\n",
    "            divisions[div].append({\"team_ID\": team_id, \"team_name\": team_name, \"roster_size\": roster_size})\n",
    "\n",
    "    return divisions\n",
    "\n",
    "\n",
    "def compute_division_targets(divisions, total_sample_size=TARGET_SAMPLE_SIZE, min_per_division=MIN_PER_DIVISION):\n",
    "    \"\"\"Compute division quotas proportional to estimated roster sizes\"\"\"\n",
    "    est_counts = {div: sum(t['roster_size'] for t in teams) for div, teams in divisions.items()}\n",
    "    total = sum(est_counts.values()) or 1\n",
    "\n",
    "    targets = {\n",
    "        div: max(int(round((count/total) * total_sample_size)), min_per_division)\n",
    "        for div, count in est_counts.items()\n",
    "    }\n",
    "\n",
    "    # normalize so sum matches target_sample_size\n",
    "    diff = total_sample_size - sum(targets.values())\n",
    "    while diff != 0:\n",
    "        for div in sorted(targets, key=lambda d: est_counts[d], reverse=True):\n",
    "            if diff > 0:\n",
    "                targets[div] += 1; diff -= 1\n",
    "            elif diff < 0 and targets[div] > min_per_division:\n",
    "                targets[div] -= 1; diff += 1\n",
    "            if diff == 0: break\n",
    "\n",
    "    print(\"Division targets:\", targets)\n",
    "    return targets\n",
    "\n",
    "\n",
    "def allocate_team_samples(teams, div_target):\n",
    "    \"\"\"Allocate swimmers to teams proportional to roster size\"\"\"\n",
    "    total_roster = sum(t['roster_size'] for t in teams) or 1\n",
    "    raw_allocs = [(t, (t['roster_size']/total_roster)*div_target) for t in teams]\n",
    "    rounded = [(t, int(round(val))) for t, val in raw_allocs]\n",
    "\n",
    "    # adjust rounding\n",
    "    diff = div_target - sum(val for _, val in rounded)\n",
    "    idx = 0\n",
    "    while diff != 0 and rounded:\n",
    "        t, val = rounded[idx % len(rounded)]\n",
    "        new_val = val + (1 if diff > 0 else -1)\n",
    "        if new_val >= 0:\n",
    "            rounded[idx % len(rounded)] = (t, new_val)\n",
    "            diff += -1 if diff > 0 else 1\n",
    "        idx += 1\n",
    "\n",
    "    return [(t, min(count, t['roster_size'])) for t, count in rounded]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e05fc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to start proportional data collection. Run the next cell to begin.\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Main Data Collection (REPLACEMENT WITH AUTO STEP SIZE)\n",
    "\n",
    "def run_data_collection_proportional():\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING PROPORTIONAL DATA COLLECTION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    start_time = time.time()\n",
    "    all_swimmers = []\n",
    "\n",
    "    # get rosters for each division\n",
    "    divisions = get_division_rosters()\n",
    "    division_targets = compute_division_targets(divisions, TARGET_SAMPLE_SIZE)\n",
    "\n",
    "    for div, div_target in division_targets.items():\n",
    "        teams = divisions[div]\n",
    "        if not teams or div_target <= 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Collecting {div_target} swimmers from {div} ---\")\n",
    "\n",
    "        if div == \"Division 1\":\n",
    "            # RANKED LIST OF TEAMS\n",
    "            ranked_teams = safe_api_call(ss.getTeamRankingsList, gender=\"M\", year=CURRENT_YEAR)\n",
    "            if not ranked_teams:\n",
    "                print(\"[WARN] Could not get Division 1 rankings, skipping\")\n",
    "                continue\n",
    "\n",
    "            n_ranked = len(ranked_teams)\n",
    "\n",
    "            # Estimate how many D1 teams to include (~10 swimmers/team)\n",
    "            est_teams_needed = max(1, div_target // 10)\n",
    "            step_size = max(1, n_ranked // est_teams_needed)\n",
    "            print(f\"D1 stratified step size = {step_size} (target ~{est_teams_needed} teams)\")\n",
    "\n",
    "            # Select every kth team\n",
    "            d1_selected_teams = ranked_teams[::step_size]\n",
    "\n",
    "            # Convert into team entries with roster sizes\n",
    "            d1_team_entries = []\n",
    "            for t in d1_selected_teams:\n",
    "                if not t or not isinstance(t, dict):\n",
    "                    print(f\"  [WARN] Invalid team object in D1 rankings, skipping\")\n",
    "                    continue\n",
    "\n",
    "                team_id, team_name = t.get(\"team_ID\"), t.get(\"team_name\")\n",
    "                if not team_id or not team_name:\n",
    "                    print(f\"  [WARN] Missing team_id/team_name in ranking entry: {t}\")\n",
    "                    continue\n",
    "\n",
    "                roster = safe_api_call(\n",
    "    ss.getRoster, team_ID=team_id, gender='M', year=CURRENT_YEAR\n",
    ")\n",
    "                if roster is None:\n",
    "                    print(f\"  [INFO] Empty or failed roster fetch for {team_name} ({team_id}), using default size\")\n",
    "                    roster_size = DEFAULT_ROSTER_SIZE\n",
    "                else:\n",
    "                    roster_size = len(roster)\n",
    "\n",
    "                d1_team_entries.append({\n",
    "                    \"team_ID\": team_id,\n",
    "                    \"team_name\": team_name,\n",
    "                    \"roster_size\": roster_size\n",
    "                })\n",
    "\n",
    "            if not d1_team_entries:\n",
    "                print(\"[WARN] No valid D1 team entries, skipping division\")\n",
    "                continue\n",
    "\n",
    "            # Allocate swimmers proportional to roster size within the sampled teams\n",
    "            team_allocs = allocate_team_samples(d1_team_entries, div_target)\n",
    "\n",
    "        else:\n",
    "            # For D2, D3, and NAIA ‚Üí proportional allocation\n",
    "            team_allocs = allocate_team_samples(teams, div_target)\n",
    "\n",
    "        # Collect swimmers according to team_allocs\n",
    "        div_swimmers = []\n",
    "        for team_entry, n in team_allocs:\n",
    "            if n <= 0:\n",
    "                continue\n",
    "\n",
    "            team_id = team_entry.get(\"team_ID\") or team_entry.get(\"teamID\")\n",
    "            team_name = team_entry.get(\"team_name\") or team_entry.get(\"teamName\", f\"Team_{team_id}\")\n",
    "\n",
    "            if not team_id:\n",
    "                print(f\"  [WARN] Missing team_id in entry: {team_entry}\")\n",
    "                continue\n",
    "\n",
    "            roster = safe_api_call(ss.getRoster, team_ID=team_id, gender=\"M\", year=CURRENT_YEAR)\n",
    "            if not roster:\n",
    "                print(f\"  [SKIP] No roster for {team_name} ({team_id})\")\n",
    "                continue\n",
    "\n",
    "            chosen = random.sample(roster, min(n, len(roster)))\n",
    "            for swimmer in chosen:\n",
    "                swimmer_data = collect_swimmer_data(swimmer, team_name, team_id, div)\n",
    "                if swimmer_data:\n",
    "                    div_swimmers.append(swimmer_data)\n",
    "                    if len(div_swimmers) >= div_target:\n",
    "                        break\n",
    "            if len(div_swimmers) >= div_target:\n",
    "                break\n",
    "\n",
    "        print(f\"‚úì {div} complete: {len(div_swimmers)} swimmers\")\n",
    "        all_swimmers.extend(div_swimmers)\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nTotal swimmers collected: {len(all_swimmers)} in {elapsed:.1f} min\")\n",
    "    return all_swimmers\n",
    "\n",
    "print(\"Ready to start proportional data collection. Run the next cell to begin.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4c6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING PROPORTIONAL DATA COLLECTION\n",
      "============================================================\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'American University', 'team_ID': 214, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Arizona State University', 'team_ID': 87, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Auburn University', 'team_ID': 127, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Ball State University', 'team_ID': 221, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Bellarmine University', 'team_ID': 10002386, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Binghamton University', 'team_ID': 383, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "An invalid team was entered, causing the following error:\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Boise State University', 'team_ID': 516, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'find_all'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Boston College', 'team_ID': 228, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Boston University', 'team_ID': 229, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "An invalid team was entered, causing the following error:\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Bowling Green State University', 'team_ID': 230, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'find_all'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Brigham Young University', 'team_ID': 220, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n",
      "[ERROR] getRoster failed. args=(), kwargs={'team': 'Brown University', 'team_ID': 17, 'gender': 'M', 'year': 2024} -> 'NoneType' object has no attribute 'text'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %% Cell 6: Execute Data Collection (REPLACEMENT)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m swimmers_raw_data = run_data_collection_proportional()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ Collection finished! Got data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(swimmers_raw_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m swimmers\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_data_collection_proportional\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m all_swimmers = []\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# get rosters for each division\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m divisions = get_division_rosters()\n\u001b[32m     13\u001b[39m division_targets = compute_division_targets(divisions, TARGET_SAMPLE_SIZE)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m div, div_target \u001b[38;5;129;01min\u001b[39;00m division_targets.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_division_rosters\u001b[39m\u001b[34m(year, gender)\u001b[39m\n\u001b[32m     13\u001b[39m team_id = t.get(\u001b[33m'\u001b[39m\u001b[33mteam_ID\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m team_name = t.get(\u001b[33m'\u001b[39m\u001b[33mteam_name\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m roster = safe_api_call(ss.getRoster, team=team_name, team_ID=team_id, gender=gender, year=year)\n\u001b[32m     16\u001b[39m roster_size = \u001b[38;5;28mlen\u001b[39m(roster) \u001b[38;5;28;01mif\u001b[39;00m roster \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_ROSTER_SIZE\n\u001b[32m     17\u001b[39m divisions[div].append({\u001b[33m\"\u001b[39m\u001b[33mteam_ID\u001b[39m\u001b[33m\"\u001b[39m: team_id, \u001b[33m\"\u001b[39m\u001b[33mteam_name\u001b[39m\u001b[33m\"\u001b[39m: team_name, \u001b[33m\"\u001b[39m\u001b[33mroster_size\u001b[39m\u001b[33m\"\u001b[39m: roster_size})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msafe_api_call\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Wrapper for API calls with error handling and debug logging\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     result = func(*args, **kwargs)\n\u001b[32m     28\u001b[39m     time.sleep(\u001b[32m0.2\u001b[39m)  \u001b[38;5;66;03m# prevent SwimCloud blocking\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\SwimScraper\\SwimScraper.py:218\u001b[39m, in \u001b[36mgetRoster\u001b[39m\u001b[34m(team, gender, team_ID, season_ID, year, pro)\u001b[39m\n\u001b[32m    214\u001b[39m \tseason_ID = \u001b[32m24\u001b[39m\n\u001b[32m    216\u001b[39m roster_url = \u001b[33m'\u001b[39m\u001b[33mhttps://www.swimcloud.com/team/\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(team_number) +  \u001b[33m'\u001b[39m\u001b[33m/roster/?page=1&gender=\u001b[39m\u001b[33m'\u001b[39m + gender + \u001b[33m'\u001b[39m\u001b[33m&season_id=\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(season_ID)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m url = requests.get(roster_url, headers = {\u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mReferer\u001b[39m\u001b[33m'\u001b[39m : \u001b[33m'\u001b[39m\u001b[33mhttps://google.com/\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m    220\u001b[39m url.encoding = \u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    222\u001b[39m soup = bs(url.text, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m, url, params=params, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m session.request(method=method, url=url, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28mself\u001b[39m.send(prep, **send_kwargs)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = adapter.send(request, **kwargs)\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = conn.urlopen(\n\u001b[32m    668\u001b[39m         method=request.method,\n\u001b[32m    669\u001b[39m         url=url,\n\u001b[32m    670\u001b[39m         body=request.body,\n\u001b[32m    671\u001b[39m         headers=request.headers,\n\u001b[32m    672\u001b[39m         redirect=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    673\u001b[39m         assert_same_host=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    674\u001b[39m         preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    675\u001b[39m         decode_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    676\u001b[39m         retries=\u001b[38;5;28mself\u001b[39m.max_retries,\n\u001b[32m    677\u001b[39m         timeout=timeout,\n\u001b[32m    678\u001b[39m         chunked=chunked,\n\u001b[32m    679\u001b[39m     )\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    788\u001b[39m     conn,\n\u001b[32m    789\u001b[39m     method,\n\u001b[32m    790\u001b[39m     url,\n\u001b[32m    791\u001b[39m     timeout=timeout_obj,\n\u001b[32m    792\u001b[39m     body=body,\n\u001b[32m    793\u001b[39m     headers=headers,\n\u001b[32m    794\u001b[39m     chunked=chunked,\n\u001b[32m    795\u001b[39m     retries=retries,\n\u001b[32m    796\u001b[39m     response_conn=response_conn,\n\u001b[32m    797\u001b[39m     preload_content=preload_content,\n\u001b[32m    798\u001b[39m     decode_content=decode_content,\n\u001b[32m    799\u001b[39m     **response_kw,\n\u001b[32m    800\u001b[39m )\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = conn.getresponse()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28msuper\u001b[39m().getresponse()\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         response.begin()\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28mself\u001b[39m._read_status()\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sock.recv_into(b)\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read(nbytes, buffer)\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carte\\miniconda3\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %% Cell 6: Execute Data Collection (REPLACEMENT)\n",
    "\n",
    "swimmers_raw_data = run_data_collection_proportional()\n",
    "\n",
    "print(f\"\\nüéâ Collection finished! Got data for {len(swimmers_raw_data)} swimmers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Data Processing and Analysis\n",
    "def process_raw_data(swimmers_raw_data):\n",
    "    \"\"\"Convert raw swimmer data to analysis-ready DataFrame\"\"\"\n",
    "    print(\"Processing raw data into analysis format...\")\n",
    "    \n",
    "    analysis_records = []\n",
    "    event_times = {event: [] for event in PRIORITY_EVENTS}\n",
    "    \n",
    "    # Flatten data structure\n",
    "    for swimmer in swimmers_raw_data:\n",
    "        for event, time_data in swimmer['times'].items():\n",
    "            time_seconds = time_data['time_seconds']\n",
    "            fina_points = calculate_fina_points(time_seconds, event)\n",
    "            \n",
    "            if fina_points and fina_points > 0:\n",
    "                record = {\n",
    "                    'swimmer_ID': swimmer['swimmer_ID'],\n",
    "                    'swimmer_name': swimmer['swimmer_name'],\n",
    "                    'team_name': swimmer['team_name'],\n",
    "                    'division': swimmer['division'],\n",
    "                    'event': event,\n",
    "                    'time_str': time_data['time_str'],\n",
    "                    'time_seconds': time_seconds,\n",
    "                    'fina_points': fina_points,\n",
    "                    'meet_name': time_data.get('meet_name', ''),\n",
    "                    'year': time_data.get('year', '')\n",
    "                }\n",
    "                analysis_records.append(record)\n",
    "                event_times[event].append(time_seconds)\n",
    "    \n",
    "    # Calculate percentiles within each event\n",
    "    for record in analysis_records:\n",
    "        event = record['event']\n",
    "        time_seconds = record['time_seconds']\n",
    "        event_times_list = event_times[event]\n",
    "        \n",
    "        if len(event_times_list) > 1:\n",
    "            # Percentile = percentage of swimmers this time beats\n",
    "            beats_count = sum(1 for t in event_times_list if t > time_seconds)\n",
    "            percentile = (beats_count / len(event_times_list)) * 100\n",
    "            record['percentile'] = round(percentile, 2)\n",
    "        else:\n",
    "            record['percentile'] = 50.0\n",
    "    \n",
    "    df = pd.DataFrame(analysis_records)\n",
    "    print(f\"‚úì Processed {len(df)} swimmer-event records\")\n",
    "    return df\n",
    "\n",
    "# Process the data\n",
    "df_analysis = process_raw_data(swimmers_raw_data)\n",
    "\n",
    "# Quick data summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total records: {len(df_analysis)}\")\n",
    "print(f\"Unique swimmers: {df_analysis['swimmer_ID'].nunique()}\")\n",
    "print(f\"Events: {list(df_analysis['event'].unique())}\")\n",
    "print(f\"Divisions: {df_analysis['division'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Statistical Analysis\n",
    "def run_statistical_analysis(df):\n",
    "    \"\"\"Perform the main statistical analysis\"\"\"\n",
    "    print(\"RUNNING STATISTICAL ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Event bias ranking\n",
    "    event_stats = df.groupby('event').agg({\n",
    "        'fina_points': ['median', 'mean', 'std', 'count'],\n",
    "        'percentile': ['median', 'mean']\n",
    "    }).round(2)\n",
    "    \n",
    "    event_stats.columns = ['median_fina', 'mean_fina', 'std_fina', 'count', \n",
    "                          'median_percentile', 'mean_percentile']\n",
    "    event_stats = event_stats.reset_index().sort_values('median_fina', ascending=False)\n",
    "    \n",
    "    print(\"EVENT BIAS RANKING (by median FINA points):\")\n",
    "    print(event_stats[['event', 'median_fina', 'count', 'median_percentile']])\n",
    "    \n",
    "    # Breaststroke vs Freestyle hypothesis test\n",
    "    breast_data = df[df['event'].str.contains('Breast')]\n",
    "    free_data = df[df['event'].str.contains('Free')]\n",
    "    \n",
    "    print(f\"\\nBREASTSTROKE HYPOTHESIS TEST:\")\n",
    "    print(f\"Breaststroke records: {len(breast_data)}\")\n",
    "    print(f\"Freestyle records: {len(free_data)}\")\n",
    "    \n",
    "    if len(breast_data) > 10 and len(free_data) > 10:\n",
    "        breast_median = breast_data['fina_points'].median()\n",
    "        free_median = free_data['fina_points'].median()\n",
    "        \n",
    "        print(f\"Breaststroke median FINA: {breast_median:.1f}\")\n",
    "        print(f\"Freestyle median FINA: {free_median:.1f}\")\n",
    "        print(f\"Difference: {breast_median - free_median:.1f} points\")\n",
    "        \n",
    "        # Statistical significance test\n",
    "        stat, p_value = stats.mannwhitneyu(\n",
    "            breast_data['fina_points'], \n",
    "            free_data['fina_points'], \n",
    "            alternative='greater'\n",
    "        )\n",
    "        \n",
    "        print(f\"Mann-Whitney U test (breast > free): p = {p_value:.6f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"‚úì SIGNIFICANT: Breaststroke FINA points are inflated!\")\n",
    "        else:\n",
    "            print(\"‚úó No significant evidence of breaststroke inflation\")\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        breast_mean = breast_data['fina_points'].mean()\n",
    "        free_mean = free_data['fina_points'].mean()\n",
    "        pooled_std = np.sqrt(((len(breast_data)-1)*breast_data['fina_points'].var() + \n",
    "                             (len(free_data)-1)*free_data['fina_points'].var()) / \n",
    "                            (len(breast_data) + len(free_data) - 2))\n",
    "        cohens_d = (breast_mean - free_mean) / pooled_std\n",
    "        print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "        \n",
    "        if abs(cohens_d) > 0.5:\n",
    "            print(\"‚úì Large effect size - practically significant difference\")\n",
    "        elif abs(cohens_d) > 0.3:\n",
    "            print(\"‚úì Medium effect size - moderate difference\")\n",
    "        else:\n",
    "            print(\"Small effect size\")\n",
    "    \n",
    "    return event_stats\n",
    "\n",
    "# Run analysis\n",
    "event_statistics = run_statistical_analysis(df_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: Visualization\n",
    "def create_analysis_plots(df, event_stats):\n",
    "    \"\"\"Create comprehensive visualization plots\"\"\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Swimming FINA Bias Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Event bias ranking\n",
    "    axes[0,0].bar(range(len(event_stats)), event_stats['median_fina'], \n",
    "                  color=['lightcoral' if 'Breast' in event else 'lightblue' \n",
    "                         for event in event_stats['event']])\n",
    "    axes[0,0].set_xticks(range(len(event_stats)))\n",
    "    axes[0,0].set_xticklabels(event_stats['event'], rotation=45, ha='right')\n",
    "    axes[0,0].set_title('Event Bias Ranking\\n(Median FINA Points)')\n",
    "    axes[0,0].set_ylabel('Median FINA Points')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Breaststroke vs Freestyle boxplot\n",
    "    breast_data = df[df['event'].str.contains('Breast')]['fina_points']\n",
    "    free_data = df[df['event'].str.contains('Free')]['fina_points']\n",
    "    \n",
    "    axes[0,1].boxplot([breast_data, free_data], \n",
    "                      labels=['Breaststroke', 'Freestyle'],\n",
    "                      patch_artist=True,\n",
    "                      boxprops=dict(facecolor='lightcoral'),\n",
    "                      medianprops=dict(color='red', linewidth=2))\n",
    "    axes[0,1].set_title('Breaststroke vs Freestyle\\nFINA Points Distribution')\n",
    "    axes[0,1].set_ylabel('FINA Points')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. FINA points vs Percentile scatter\n",
    "    colors = {'100 Breast': 'red', '200 Breast': 'darkred', \n",
    "              '100 Free': 'blue', '200 Free': 'darkblue'}\n",
    "    \n",
    "    for event in df['event'].unique():\n",
    "        event_data = df[df['event'] == event]\n",
    "        if len(event_data) > 5:\n",
    "            axes[0,2].scatter(event_data['percentile'], event_data['fina_points'],\n",
    "                            alpha=0.6, label=event, s=30, c=colors.get(event, 'gray'))\n",
    "    \n",
    "    axes[0,2].set_xlabel('Percentile Rank')\n",
    "    axes[0,2].set_ylabel('FINA Points')\n",
    "    axes[0,2].set_title('FINA Points vs Percentile\\nby Event')\n",
    "    axes[0,2].legend(fontsize=8)\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Division distribution\n",
    "    div_counts = df['division'].value_counts()\n",
    "    colors_div = plt.cm.Set3(np.linspace(0, 1, len(div_counts)))\n",
    "    axes[1,0].pie(div_counts.values, labels=div_counts.index, autopct='%1.1f%%',\n",
    "                  colors=colors_div)\n",
    "    axes[1,0].set_title('Sample Distribution\\nby Division')\n",
    "    \n",
    "    # 5. Sample sizes by event\n",
    "    event_counts = df['event'].value_counts()\n",
    "    bars = axes[1,1].bar(range(len(event_counts)), event_counts.values,\n",
    "                        color=['lightcoral' if 'Breast' in event else 'lightblue' \n",
    "                               for event in event_counts.index])\n",
    "    axes[1,1].set_xticks(range(len(event_counts)))\n",
    "    axes[1,1].set_xticklabels(event_counts.index, rotation=45, ha='right')\n",
    "    axes[1,1].set_title('Sample Sizes by Event')\n",
    "    axes[1,1].set_ylabel('Number of Swimmers')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                       f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 6. Time distribution comparison\n",
    "    df_breast_100 = df[df['event'] == '100 Breast']['time_seconds']\n",
    "    df_free_100 = df[df['event'] == '100 Free']['time_seconds']\n",
    "    \n",
    "    axes[1,2].hist(df_breast_100, alpha=0.7, label='100 Breast', \n",
    "                   color='lightcoral', bins=15)\n",
    "    axes[1,2].hist(df_free_100, alpha=0.7, label='100 Free', \n",
    "                   color='lightblue', bins=15)\n",
    "    axes[1,2].set_xlabel('Time (seconds)')\n",
    "    axes[1,2].set_ylabel('Frequency')\n",
    "    axes[1,2].set_title('Time Distribution\\n100 Breast vs 100 Free')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fina_bias_analysis_complete.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "create_analysis_plots(df_analysis, event_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c37010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Save Results and Summary\n",
    "def save_results_and_summary(df, event_stats, swimmers_raw):\n",
    "    \"\"\"Save all results and create summary report\"\"\"\n",
    "    \n",
    "    # Save main datasets\n",
    "    df.to_csv('swimming_fina_analysis.csv', index=False)\n",
    "    event_stats.to_csv('event_bias_statistics.csv', index=False)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_report = f\"\"\"\n",
    "SWIMMING FINA BIAS ANALYSIS - SUMMARY REPORT\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "============================================\n",
    "\n",
    "SAMPLE CHARACTERISTICS:\n",
    "- Total swimmer-event records: {len(df)}\n",
    "- Unique swimmers: {df['swimmer_ID'].nunique()}\n",
    "- Events analyzed: {', '.join(df['event'].unique())}\n",
    "- Divisions represented: {', '.join(df['division'].unique())}\n",
    "\n",
    "DIVISION BREAKDOWN:\n",
    "{df['division'].value_counts().to_string()}\n",
    "\n",
    "EVENT SAMPLE SIZES:\n",
    "{df['event'].value_counts().to_string()}\n",
    "\n",
    "KEY FINDINGS:\n",
    "============\n",
    "\n",
    "EVENT BIAS RANKING (by median FINA points):\n",
    "{event_stats[['event', 'median_fina', 'count']].to_string(index=False)}\n",
    "\n",
    "HYPOTHESIS TEST RESULTS:\n",
    "{'-' * 30}\n",
    "\"\"\"\n",
    "    \n",
    "    # Add hypothesis test results\n",
    "    breast_data = df[df['event'].str.contains('Breast')]['fina_points']\n",
    "    free_data = df[df['event'].str.contains('Free')]['fina_points']\n",
    "    \n",
    "    if len(breast_data) > 10 and len(free_data) > 10:\n",
    "        stat, p_value = stats.mannwhitneyu(breast_data, free_data, alternative='greater')\n",
    "        \n",
    "        summary_report += f\"\"\"\n",
    "Breaststroke median FINA: {breast_data.median():.1f}\n",
    "Freestyle median FINA: {free_data.median():.1f}\n",
    "Difference: {breast_data.median() - free_data.median():.1f} points\n",
    "\n",
    "Statistical Test (Mann-Whitney U):\n",
    "P-value: {p_value:.6f}\n",
    "Result: {'SIGNIFICANT BIAS DETECTED' if p_value < 0.05 else 'NO SIGNIFICANT BIAS'}\n",
    "\n",
    "Effect Size (Cohen's d): {((breast_data.mean() - free_data.mean()) / \n",
    "                          np.sqrt(((len(breast_data)-1)*breast_data.var() + \n",
    "                                  (len(free_data)-1)*free_data.var()) / \n",
    "                                 (len(breast_data) + len(free_data) - 2))):.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    summary_report += f\"\"\"\n",
    "\n",
    "CONCLUSIONS:\n",
    "============\n",
    "{'‚úì HYPOTHESIS SUPPORTED: Breaststroke events show inflated FINA points' \n",
    " if len(breast_data) > 10 and len(free_data) > 10 and \n",
    "    stats.mannwhitneyu(breast_data, free_data, alternative='greater')[1] < 0.05\n",
    " else '‚úó HYPOTHESIS NOT SUPPORTED: No significant bias detected'}\n",
    "\n",
    "FILES GENERATED:\n",
    "- swimming_fina_analysis.csv (detailed results)\n",
    "- event_bias_statistics.csv (summary statistics)\n",
    "- fina_bias_analysis_complete.png (visualizations)\n",
    "- analysis_summary_report.txt (this report)\n",
    "\"\"\"\n",
    "    \n",
    "    # Save summary report\n",
    "    with open('analysis_summary_report.txt', 'w') as f:\n",
    "        f.write(summary_report)\n",
    "    \n",
    "    print(\"Results saved successfully!\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    print(\"- swimming_fina_analysis.csv\")\n",
    "    print(\"- event_bias_statistics.csv\") \n",
    "    print(\"- fina_bias_analysis_complete.png\")\n",
    "    print(\"- analysis_summary_report.txt\")\n",
    "    \n",
    "    return summary_report\n",
    "\n",
    "# Save all results\n",
    "final_summary = save_results_and_summary(df_analysis, event_statistics, swimmers_raw_data)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fd884",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Analysis Complete!\n",
    "# \n",
    "# This notebook has successfully:\n",
    "# 1. ‚úÖ Collected stratified sample data from NCAA/NAIA divisions\n",
    "# 2. ‚úÖ Calculated FINA points and percentiles for each swimmer-event\n",
    "# 3. ‚úÖ Tested the breaststroke inflation hypothesis\n",
    "# 4. ‚úÖ Generated comprehensive visualizations\n",
    "# 5. ‚úÖ Saved results for further analysis\n",
    "# \n",
    "# ### Key Takeaways:\n",
    "# - Review the statistical test results above\n",
    "# - Check the visualizations for patterns\n",
    "# - Examine the saved CSV files for detailed data\n",
    "# \n",
    "# ### Next Steps:\n",
    "# - Validate results with official FINA base times\n",
    "# - Expand analysis to include more events\n",
    "# - Consider temporal trends in bias patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
